Loading and sampling datasets...
Loaded 300 samples from CNN/DailyMail.
Loaded 300 samples from SQuAD v2.
Processed 300 samples from Quora.
Loading evaluation metrics...
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/maths/btech/mt1210235/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/maths/btech/mt1210235/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /home/maths/btech/mt1210235/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
NLTK data (wordnet, punkt, omw-1.4) found.

===== Processing Model: Qwen (Qwen/Qwen2.5-1.5B) =====
Loading Qwen...
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
   - Model and Tokenizer loaded.

--- Task: summarization ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 4.9863 sec/sample.
Calculating metrics...
Metrics for Qwen on summarization: {'avg_inference_time_sec': 4.986250600814819, 'ROUGE-L': np.float64(0.1841317917016517)}

--- Task: qa ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 0.6181 sec/sample.
Calculating metrics...
Preparing data for BERTScore...
Calculating BERTScore for 216 samples with answers...
Using device cuda:0 for BERTScore calculation.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
BERTScore calculated successfully.
Metrics for Qwen on qa: {'avg_inference_time_sec': 0.6180549200375874, 'ROUGE-L': np.float64(0.477486607218519), 'BERTScore_F1': 0.8996005069326471}

--- Task: paraphrase ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 1.1774 sec/sample.
Calculating metrics...
Metrics for Qwen on paraphrase: {'avg_inference_time_sec': 1.1773664999008178, 'SacreBLEU': 5.112373405835537, 'METEOR': np.float64(0.33856177217467986)}

Unloading Qwen to free memory...
   - Memory cleared.

===== Processing Model: OPT (facebook/opt-1.3b) =====
Loading OPT...
   - Model and Tokenizer loaded.

--- Task: summarization ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 4.2900 sec/sample.
Calculating metrics...
Metrics for OPT on summarization: {'avg_inference_time_sec': 4.29003715356191, 'ROUGE-L': np.float64(0.17823300518703872)}

--- Task: qa ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 0.8755 sec/sample.
Calculating metrics...
Preparing data for BERTScore...
Calculating BERTScore for 216 samples with answers...
Using device cuda:0 for BERTScore calculation.
BERTScore calculated successfully.
Metrics for OPT on qa: {'avg_inference_time_sec': 0.8754738306999207, 'ROUGE-L': np.float64(0.06660397987268658), 'BERTScore_F1': 0.8277439425388972}

--- Task: paraphrase ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 1.6140 sec/sample.
Calculating metrics...
Metrics for OPT on paraphrase: {'avg_inference_time_sec': 1.6140290149052938, 'SacreBLEU': 2.470620715135642, 'METEOR': np.float64(0.25188234410869503)}

Unloading OPT to free memory...
   - Memory cleared.

===== Processing Model: Llama (meta-llama/Llama-3.2-1B) =====
Loading Llama...
   - Added EOS token as PAD token.
   - Model and Tokenizer loaded.

--- Task: summarization ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 4.7302 sec/sample.
Calculating metrics...
Metrics for Llama on summarization: {'avg_inference_time_sec': 4.730222744146983, 'ROUGE-L': np.float64(0.17638109361136417)}

--- Task: qa ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 0.8269 sec/sample.
Calculating metrics...
Preparing data for BERTScore...
Calculating BERTScore for 216 samples with answers...
Using device cuda:0 for BERTScore calculation.
Warning: Empty candidate sentence detected; setting raw BERTscores to 0.
Warning: Empty candidate sentence detected; setting raw BERTscores to 0.
BERTScore calculated successfully.
Metrics for Llama on qa: {'avg_inference_time_sec': 0.8268547360102335, 'ROUGE-L':np.float64(0.14448045975363233), 'BERTScore_F1': 0.8382268352089105}

--- Task: paraphrase ---
Generating 300 predictions...
   Processed 20/300 samples...
   Processed 40/300 samples...
   Processed 60/300 samples...
   Processed 80/300 samples...
   Processed 100/300 samples...
   Processed 120/300 samples...
   Processed 140/300 samples...
   Processed 160/300 samples...
   Processed 180/300 samples...
   Processed 200/300 samples...
   Processed 220/300 samples...
   Processed 240/300 samples...
   Processed 260/300 samples...
   Processed 280/300 samples...
   Processed 300/300 samples...
Generation finished. Average inference time: 1.8863 sec/sample.
Calculating metrics...
Metrics for Llama on paraphrase: {'avg_inference_time_sec': 1.886312922636668, 'SacreBLEU': 2.286600048244479, 'METEOR': np.float64(0.25735991754252047)}

Unloading Llama to free memory...
   - Memory cleared.


===== Baseline Performance Summary =====

--- Model: Qwen ---
  Task: summarization
    avg_inference_time_sec: 4.9863
    ROUGE-L: 0.1841
  Task: qa
    avg_inference_time_sec: 0.6181
    ROUGE-L: 0.4775
    BERTScore_F1: 0.8996
  Task: paraphrase
    avg_inference_time_sec: 1.1774
    SacreBLEU: 5.1124
    METEOR: 0.3386

--- Model: OPT ---
  Task: summarization
    avg_inference_time_sec: 4.2900
    ROUGE-L: 0.1782
  Task: qa
    avg_inference_time_sec: 0.8755
    ROUGE-L: 0.0666
    BERTScore_F1: 0.8277
  Task: paraphrase
    avg_inference_time_sec: 1.6140
    SacreBLEU: 2.4706
    METEOR: 0.2519

--- Model: Llama ---
  Task: summarization
    avg_inference_time_sec: 4.7302
    ROUGE-L: 0.1764
  Task: qa
    avg_inference_time_sec: 0.8269
    ROUGE-L: 0.1445
    BERTScore_F1: 0.8382
  Task: paraphrase
    avg_inference_time_sec: 1.8863
    SacreBLEU: 2.2866
    METEOR: 0.2574

Baseline evaluation complete.



Comments from Gemini:


Overall Impression:

Confirmation of Baseline Status: As expected for zero-shot performance with models in the ~1.5B-8B parameter range (assuming Llama is 8B, or comparing 1B-1.5B models), the absolute scores are generally modest, especially on summarization and paraphrase tasks requiring significant generation quality. This reinforces the need for fine-tuning or more advanced multi-model strategies.

Increased Stability: Using 300 samples provides more confidence in these scores compared to 100. They should be reasonably representative of the models' baseline capabilities.

Model Differences Emerging: Clear performance differences between the models are apparent, especially Qwen compared to OPT and Llama in this setup.

BERTScore Fix: The successful calculation of BERTScore for QA provides crucial semantic insight that ROUGE-L alone was missing.


Task-Specific Comments:

Summarization (CNN/DailyMail - ROUGE-L):
Scores: Qwen (0.184), OPT (0.178), Llama (0.176).

Comments: Very consistent scores across all three models, hovering around 0.17-0.18. This is typical for zero-shot abstractive summarization with models this size. It suggests they capture some keywords/phrases but struggle to generate fluent, novel summaries matching the reference closely. Qwen holds a very slight edge. Fine-tuning will be key for improvement here.

Question Answering (SQuAD v2 - ROUGE-L & BERTScore_F1):
ROUGE-L: Qwen (0.477), OPT (0.067), Llama (0.144).
BERTScore_F1: Qwen (0.900), OPT (0.828), Llama (0.838). (Calculated on ~216/300 samples with answers).

Comments:
Metric Discrepancy: The huge difference between ROUGE-L and BERTScore is striking.
BERTScore: The high BERTScF1 scores (especially Qwen's ~0.90) indicate that when an answer is present in the context, all models generate responses that are semantically very similar to the ground truth answers, even if the wording differs. This is a good sign for their comprehension ability.
ROUGE-L: Qwen's extremely high ROUGE-L suggests it might be better at extractive-like answering, often repeating phrases from the context that match the answer, or generating more lexically similar answers. OPT's and Llama's very low ROUGE-L, despite decent BERTScore, implies they generate semantically correct answers but with significantly different phrasing, hurting lexical overlap scores. They might also struggle more with generating the correct format or handling unanswerable questions gracefully (though this isn't directly measured by BERTScore on answered questions).
Overall QA: Qwen appears significantly stronger on QA by both metrics in this baseline test. OPT seems particularly weak based on ROUGE-L.

Paraphrase Generation (Quora - SacreBLEU & METEOR):
SacreBLEU: Qwen (5.11), OPT (2.47), Llama (2.29).
METEOR: Qwen (0.339), OPT (0.252), Llama (0.257).

Comments: Both metrics show that zero-shot paraphrase generation is challenging.
SacreBLEU: The low scores (especially for OPT/Llama) indicate generated paraphrases have poor lexical overlap with the single reference paraphrase. This isn't surprising zero-shot.
METEOR: Scores are higher, suggesting some semantic similarity (synonyms, stemming).
Qwen's Lead: Qwen shows a clear and significant advantage over OPT and Llama on both metrics for this task, suggesting it's better at capturing the meaning and generating related phrasing, even if imperfect. OPT and Llama perform similarly poorly.


Model-Specific Summary:

Qwen: Emerges as the strongest baseline model across all three tasks in this evaluation, particularly dominant in QA and Paraphrase generation according to these metrics.

OPT: Performs reasonably on summarization (similar to others) but struggles significantly on QA (based on ROUGE-L) and Paraphrase compared to Qwen.

Llama: Performs similarly to OPT on Summarization and Paraphrase. Its QA performance is weak on ROUGE-L but decent on BERTScore (semantic similarity). It consistently underperforms Qwen in this baseline.


Next Steps:
These baseline scores are valuable. They clearly show which models have initial strengths/weaknesses on which tasks and provide a quantitative measure to beat with your fine-tuning and multi-model system design efforts. Qwen seems like a strong candidate for tasks like QA and Paraphrase, while all models need significant improvement for Summarization.
